{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ed3ed99-e207-4c33-b244-97bf3271fdd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Improved Accuracies (Optimized) ===\n",
      "Logistic Regression: 0.8260 (best params: {'classifier__C': 1})\n",
      "SVM: 0.8294 (best params: {'classifier__C': 1, 'classifier__kernel': 'rbf'})\n",
      "KNN: 0.8137 (best params: {'classifier__n_neighbors': 5})\n",
      "Random Forest: 0.8350 (best params: {'classifier__max_depth': 6, 'classifier__n_estimators': 100})\n",
      "Naive Bayes: 0.7677\n",
      "Perceptron: 0.7396\n",
      "SGD: 0.7667\n",
      "Linear SVC: 0.8260\n",
      "Decision Tree: 0.8148 (best params: {'classifier__max_depth': 3})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron, SGDClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "train = pd.read_csv(r\"C:\\Users\\punna\\Downloads\\titanic\\train.csv\")\n",
    "test = pd.read_csv(r\"C:\\Users\\punna\\Downloads\\titanic\\train.csv\")\n",
    "gender = pd.read_csv(r\"C:\\Users\\punna\\Downloads\\titanic\\train.csv\")  # only used for Kaggle submission, not training\n",
    "\n",
    "for df in [train, test]:\n",
    "    # Extract Title\n",
    "    df[\"Title\"] = df[\"Name\"].str.extract(\" ([A-Za-z]+)\\.\", expand=False)\n",
    "    df[\"Title\"] = df[\"Title\"].replace([\"Lady\",\"Countess\",\"Capt\",\"Col\",\"Don\",\"Dr\",\n",
    "                                       \"Major\",\"Rev\",\"Sir\",\"Jonkheer\",\"Dona\"], \"Rare\")\n",
    "    df[\"Title\"] = df[\"Title\"].replace({\"Mlle\":\"Miss\",\"Ms\":\"Miss\",\"Mme\":\"Mrs\"})\n",
    "    \n",
    "   \n",
    "    df[\"FamilySize\"] = df[\"SibSp\"] + df[\"Parch\"] + 1\n",
    "    df[\"IsAlone\"] = (df[\"FamilySize\"] == 1).astype(int)\n",
    "    \n",
    "    df[\"FarePerPerson\"] = df[\"Fare\"] / df[\"FamilySize\"]\n",
    "    \n",
    "    df[\"Deck\"] = df[\"Cabin\"].astype(str).str[0]\n",
    "\n",
    "\n",
    "train = train.drop([\"PassengerId\",\"Name\",\"Ticket\",\"Cabin\"], axis=1)\n",
    "test_passenger_ids = test[\"PassengerId\"]\n",
    "test = test.drop([\"PassengerId\",\"Name\",\"Ticket\",\"Cabin\"], axis=1)\n",
    "\n",
    "\n",
    "X = train.drop(\"Survived\", axis=1)\n",
    "y = train[\"Survived\"]\n",
    "\n",
    "categorical_cols = [\"Sex\", \"Embarked\", \"Title\", \"Deck\"]\n",
    "numeric_cols = [col for col in X.columns if col not in categorical_cols]\n",
    "\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", KNNImputer(n_neighbors=5)),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_cols),\n",
    "        (\"cat\", categorical_transformer, categorical_cols)\n",
    "    ])\n",
    "\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"SVM\": SVC(),\n",
    "    \"KNN\": KNeighborsClassifier(),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"Naive Bayes\": GaussianNB(),\n",
    "    \"Perceptron\": Perceptron(),\n",
    "    \"SGD\": SGDClassifier(max_iter=1000, tol=1e-3),\n",
    "    \"Linear SVC\": LinearSVC(max_iter=2000),\n",
    "    \"Decision Tree\": DecisionTreeClassifier()\n",
    "}\n",
    "\n",
    "param_grids = {\n",
    "    \"Logistic Regression\": {\"classifier__C\":[0.1,1,10]},\n",
    "    \"SVM\": {\"classifier__C\":[0.5,1,10], \"classifier__kernel\":[\"rbf\",\"linear\"]},\n",
    "    \"KNN\": {\"classifier__n_neighbors\":[3,5,7,9]},\n",
    "    \"Random Forest\": {\"classifier__n_estimators\":[100,200], \"classifier__max_depth\":[4,6,8]},\n",
    "    \"Decision Tree\": {\"classifier__max_depth\":[3,5,7,9]}\n",
    "}\n",
    "\n",
    "print(\"\\n=== Improved Accuracies (Optimized) ===\")\n",
    "for name, model in models.items():\n",
    "    pipe = Pipeline(steps=[(\"preprocessor\", preprocessor),\n",
    "                           (\"classifier\", model)])\n",
    "    \n",
    "    if name in param_grids:\n",
    "        grid = GridSearchCV(pipe, param_grids[name], cv=5,\n",
    "                            scoring=\"accuracy\", n_jobs=-1)\n",
    "        grid.fit(X, y)\n",
    "        print(f\"{name}: {grid.best_score_:.4f} (best params: {grid.best_params_})\")\n",
    "    else:\n",
    "        scores = cross_val_score(pipe, X, y, cv=5, scoring=\"accuracy\")\n",
    "        print(f\"{name}: {scores.mean():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ca0c012-31f0-4935-9cae-22af1614875b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Decision Tree CV accuracy:  0.8361245370660976\n",
      "Best DT parameters: {'classifier__criterion': 'gini', 'classifier__max_depth': 5, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 10}\n",
      "Decision tree plot saved to: C:\\Users\\punna\\Downloads\\titanic\\decision_tree_preview.png\n",
      "\n",
      "Best Random Forest CV accuracy:  0.855200552382148\n",
      "Best RF parameters: {'classifier__max_depth': 10, 'classifier__max_features': None, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 10, 'classifier__n_estimators': 400}\n",
      "\n",
      "=== Model Comparison (5-Fold CV Accuracy) ===\n",
      "Decision Tree: 0.8361\n",
      "Random Forest: 0.8552\n",
      "→ Random Forest performs better due to reduced variance and ensemble averaging.\n"
     ]
    }
   ],
   "source": [
    "# =====================\n",
    "# === Extension Cell (Full) ===\n",
    "# =====================\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.tree import plot_tree, DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "csv_dir = r\"C:\\Users\\punna\\Downloads\\titanic\"\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_cols),\n",
    "        (\"cat\", categorical_transformer, categorical_cols)\n",
    "    ])\n",
    "\n",
    "\n",
    "dt_pipeline = Pipeline(steps=[\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"classifier\", DecisionTreeClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "param_grid_dt = {\n",
    "    \"classifier__criterion\": [\"gini\", \"entropy\", \"log_loss\"],\n",
    "    \"classifier__max_depth\": [3, 5, 7, 9, None],\n",
    "    \"classifier__min_samples_split\": [2, 5, 10],\n",
    "    \"classifier__min_samples_leaf\": [1, 2, 4],\n",
    "}\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "grid_dt = GridSearchCV(dt_pipeline, param_grid_dt, cv=cv,\n",
    "                       scoring=\"accuracy\", n_jobs=-1, refit=True)\n",
    "grid_dt.fit(X, y)\n",
    "\n",
    "print(\"\\nBest Decision Tree CV accuracy: \", grid_dt.best_score_)\n",
    "print(\"Best DT parameters:\", grid_dt.best_params_)\n",
    "\n",
    "\n",
    "best_dt = grid_dt.best_estimator_\n",
    "ohe = best_dt.named_steps[\"preprocessor\"].named_transformers_[\"cat\"].named_steps[\"encoder\"]\n",
    "feature_names = np.concatenate([\n",
    "    numeric_cols,\n",
    "    ohe.get_feature_names_out(categorical_cols)\n",
    "])\n",
    "\n",
    "# Save plot in the same directory as Titanic CSV files\n",
    "plot_path = os.path.join(csv_dir, \"decision_tree_preview.png\")\n",
    "\n",
    "plt.figure(figsize=(22, 12))\n",
    "plot_tree(best_dt.named_steps[\"classifier\"],\n",
    "          feature_names=feature_names,\n",
    "          class_names=[\"Not Survived\", \"Survived\"],\n",
    "          filled=True, rounded=True, max_depth=3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(plot_path, dpi=200)\n",
    "plt.close()\n",
    "print(f\"Decision tree plot saved to: {plot_path}\")\n",
    "\n",
    "\n",
    "rf_pipeline = Pipeline(steps=[\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"classifier\", RandomForestClassifier(random_state=42, n_jobs=-1))\n",
    "])\n",
    "\n",
    "param_grid_rf = {\n",
    "    \"classifier__n_estimators\": [200, 400],\n",
    "    \"classifier__max_depth\": [None, 6, 10],\n",
    "    \"classifier__min_samples_split\": [2, 5, 10],\n",
    "    \"classifier__min_samples_leaf\": [1, 2, 4],\n",
    "    \"classifier__max_features\": [\"sqrt\", None],\n",
    "}\n",
    "\n",
    "grid_rf = GridSearchCV(rf_pipeline, param_grid_rf, cv=cv,\n",
    "                       scoring=\"accuracy\", n_jobs=-1, refit=True)\n",
    "grid_rf.fit(X, y)\n",
    "\n",
    "print(\"\\nBest Random Forest CV accuracy: \", grid_rf.best_score_)\n",
    "print(\"Best RF parameters:\", grid_rf.best_params_)\n",
    "\n",
    "\n",
    "print(\"\\n=== Model Comparison (5-Fold CV Accuracy) ===\")\n",
    "print(f\"Decision Tree: {grid_dt.best_score_:.4f}\")\n",
    "print(f\"Random Forest: {grid_rf.best_score_:.4f}\")\n",
    "\n",
    "if grid_rf.best_score_ > grid_dt.best_score_:\n",
    "    print(\"→ Random Forest performs better due to reduced variance and ensemble averaging.\")\n",
    "else:\n",
    "    print(\"→ Decision Tree performs similarly or slightly better; check overfitting or data size.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3fd2d7-5ee8-4bfd-b524-61a4e9387c83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
